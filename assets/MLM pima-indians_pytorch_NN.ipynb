{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76e7e21",
   "metadata": {},
   "source": [
    "# Develop Your First Neural Network with PyTorch, Step by Step (Last Updated on April 8, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af0841",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/#:~:text=The%20steps%20you%20will%20learn%20in%20this%20post,a%20Training%20Loop%20Evaluate%20the%20Model%20Make%20Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720e42c",
   "metadata": {},
   "source": [
    "## PyTorch is a powerful Python library for building deep learning models. \n",
    "\n",
    "It provides everything you need to define and train a neural network and use it for inference. \n",
    "You don’t need to write much code to complete all this. \n",
    "In this pose, you will discover how to create your first deep learning neural network model in Python\n",
    "using PyTorch. After completing this post, you will know:\n",
    "\n",
    "-How to load a CSV dataset and prepare it for use with PyTorch\n",
    "-How to define a Multilayer Perceptron model in PyToch\n",
    "-How to train and evaluate a PyToch model on a validation dataset\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba24738",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "There is not a lot of code required.\n",
    "You will go over it slowly so that you will know how to create your own models in the future. \n",
    "The steps you will learn in this post are as follows:\n",
    "\n",
    "-Load Data\n",
    "-Define PyToch Model\n",
    "-Define Loss Function and Optimizers\n",
    "-Run a Training Loop\n",
    "-Evaluate the Model\n",
    "-Make Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61b470",
   "metadata": {},
   "source": [
    "Load Data\n",
    "\n",
    "The first step is to define the functions and classes you intend to use in this post. \n",
    "You will use the NumPy library to load your dataset and the  PyTorch library for deep learning models.\n",
    "\n",
    "The imports required are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7c29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba2742",
   "metadata": {},
   "source": [
    "You can now load your dataset.\n",
    "\n",
    "In this post, you will use the Pima Indians onset of diabetes dataset. This has been a standard machine learning dataset since the early days of the field. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
    "\n",
    "It is a binary classification problem (onset of diabetes as 1 or not as 0). All the input variables that describe each patient are transformed and numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our first neural network in PyTorch.\n",
    "\n",
    "You can also download it here.\n",
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
    "\n",
    "Download the dataset and place it in your local working directory, the same location as your Python file. Save it with the filename pima-indians-diabetes.csv. Take a look inside the file; you should see rows of data like the following:\n",
    "\n",
    "6,148,72,35,0,33.6,0.627,50,1\n",
    "1,85,66,29,0,26.6,0.351,31,0\n",
    "8,183,64,0,0,23.3,0.672,32,1\n",
    "1,89,66,23,94,28.1,0.167,21,0\n",
    "0,137,40,35,168,43.1,2.288,33,1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71cc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ae307c4",
   "metadata": {},
   "source": [
    "You can now load the file as a matrix of numbers using the NumPy function loadtxt(). There are eight input variables and one output variable (the last column). You will be learning a model to map rows of input variables (X) to an output variable (y), which is often summarized as y=f(x). The variables are summarized as follows:\n",
    "\n",
    "Input Variables (X):\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-hour serum insulin (μIU/ml)\n",
    "6. Body mass index (weight in kg/(height in m)2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "\n",
    "- Class label (0 or 1)\n",
    "\n",
    "Once the CSV file is loaded into memory, you can split the columns of data into input and output variables.\n",
    "\n",
    "The data will be stored in a 2D array where the first dimension is rows and the second dimension is columns, e.g., (rows, columns). You can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator “:“ \n",
    "\n",
    "(ek: doesn't this represent all rows?).\n",
    "\n",
    "You can select the first eight columns from index 0 to index 7 via the slice 0:8. You can then select the output column (the 9th variable) via index 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abaf351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('C:/Users/Evan Kwan/Desktop/Python/pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8fc96",
   "metadata": {},
   "source": [
    "But these data should be converted to PyTorch tensors first. One reason is that PyTorch usually operates in a 32-bit floating point while NumPy, by default, uses a 64-bit floating point. Mix-and-match is not allowed in most operations. Converting to PyTorch tensors can avoid the implicit conversion that may cause problems. You can also take this chance to correct the shape to fit what PyTorch would expect, e.g., prefer n x 1 matrix over n-vectors.\n",
    "\n",
    "To convert, create a tensor out of NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a029b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490cada5",
   "metadata": {},
   "source": [
    "You are now ready to define your neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c395f4",
   "metadata": {},
   "source": [
    "Define the Model\n",
    "\n",
    "Indeed, there are two ways to define a model in PyTorch. The goal is to make it like a function that takes an input and returns an output.\n",
    "\n",
    "A model can be defined as a sequence of layers. You create a Sequential model with the layers listed out. The first thing you need to do to get this right is to ensure the first layer has the correct number of input features. In this example, you can specify the input dimension  8 for the eight input variables as one vector.\n",
    "\n",
    "The other parameters for a layer or how many layers you need for a model is not an easy question. You may use heuristics to help you design the model, or you can refer to other people’s designs in dealing with a similar problem. Often, the best neural network structure is found through a process of trial-and-error experimentation. Generally, you need a network large enough to capture the structure of the problem but small enough to make it fast. In this example, let’s use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers or dense layers are defined using the Linear class in PyTorch. It simply means an operation similar to matrix multiplication. You can specify the number of inputs as the first argument and the number of outputs as the second argument. The number of outputs is sometimes called the number of neurons or number of nodes in the layer.\n",
    "\n",
    "You also need an activation function after the layer. If not provided, you just take the output of the matrix multiplication to the next step, or sometimes you call it using linear activation, hence the name of the layer.\n",
    "\n",
    "In this example, you will use the rectified linear unit activation function, referred to as ReLU, on the first two layers and the sigmoid function in the output layer.\n",
    "\n",
    "A sigmoid on the output layer ensures the output is between 0 and 1, which is easy to map to either a probability of class 1 or snap to a hard classification of either class by a cut-off threshold of 0.5. In the past, you might have used sigmoid and tanh activation functions for all layers, but it turns out that sigmoid activation can lead to the problem of vanishing gradient in deep neural networks, and ReLU activation is found to provide better performance in terms of both speed and accuracy.\n",
    "\n",
    "You can piece it all together by adding each layer such that:\n",
    "-The model expects rows of data with 8 variables (the first argument at the first layer set to 8)\n",
    "-The first hidden layer has 12 neurons, followed by a ReLU activation function\n",
    "-The second hidden layer has 8 neurons, followed by another ReLU activation function\n",
    "-The output layer has one neuron, followed by a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497f55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0efa87",
   "metadata": {},
   "source": [
    "You can check the model by printing it out as follows:\n",
    "\n",
    "You will see:\n",
    "Sequential(\n",
    "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
    "  (1): ReLU()\n",
    "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
    "  (3): ReLU()\n",
    "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
    "  (5): Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cde0821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c4ebf",
   "metadata": {},
   "source": [
    "You are free to change the design and see if you get a better or worse result than the subsequent part of this post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "843e3c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# But note that, in PyTorch, there is a more verbose way of creating a model. \n",
    "# The model above can be created as a Python class inherited from the nn.Module:\n",
    "\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        # note the 12 above corresponds to 12 below, and leads to 8\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        # note the 8 above corresponds to the 8 below, and leads to 1 just for Sigmoid (binary) output\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = PimaClassifier()\n",
    "print(model)\n",
    "\n",
    "# In this case, the model printed will be:\n",
    "# PimaClassifier(\n",
    "#   (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
    "#   (act1): ReLU()\n",
    "#   (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
    "#   (act2): ReLU()\n",
    "#   (output): Linear(in_features=8, out_features=1, bias=True)\n",
    "#   (act_output): Sigmoid()\n",
    "# )\n",
    "\n",
    "\n",
    "# In this approach, a class needs to have all the layers defined in the constructor because\n",
    "# you need to prepare all its components when it is created, but the input is not yet provided. \n",
    "# Note that you also need to call the parent class’s constructor (the line super().__init__()) \n",
    "# to bootstrap your model. You also need to define a forward() function in the class to tell, \n",
    "# if an input tensor x is provided, how you produce the output tensor in return.\n",
    "\n",
    "# You can see from the output above that the model remembers how you call each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe389fa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6dc9eb5",
   "metadata": {},
   "source": [
    "# Preparation for Training\n",
    "A defined model is ready for training, but you need to specify what the goal of the training is. In this example, the data has the input features X and the output label y. You want the neural network model to produce an output that is as close to y as possible. Training a network means finding the best set of weights to map inputs to outputs in your dataset. The loss function is the metric to measure the prediction’s distance to y. In this example, you should use binary cross entropy because it is a binary classification problem.\n",
    "\n",
    "Once you decide on the loss function, you also need an optimizer. The optimizer is the algorithm you use to adjust the model weights progressively to produce a better output. There are many optimizers to choose from, and in this example, Adam is used. This popular version of gradient descent can automatically tune itself and gives good results in a wide range of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3122dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f56dd",
   "metadata": {},
   "source": [
    "The optimizer usually has some configuration parameters. Most notably, the learning rate lr. But all optimizers need to know what to optimize. Therefore. you pass on model.parameters(), which is a generator of all parameters from the model you created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4a902",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "You have defined your model, the loss metric, and the optimizer. It is ready for training by executing the model on some data.\n",
    "\n",
    "Training a neural network model usually takes in epochs and batches. They are idioms for how data is passed to a model:\n",
    "\n",
    "Epoch: Passes the entire training dataset to the model once\n",
    "Batch: One or more samples passed to the model, from which the gradient descent algorithm will be executed for one iteration\n",
    "Simply speaking, the entire dataset is split into batches, and you pass the batches one by one into a model using a training loop. Once you have exhausted all the batches, you have finished one epoch. Then you can start over again with the same dataset and start the second epoch, continuing to refine the model. This process repeats until you are satisfied with the model’s output.\n",
    "\n",
    "The size of a batch is limited by the system’s memory. Also, the number of computations required is linearly proportional to the size of a batch. The total number of batches over many epochs is how many times you run the gradient descent to refine the model. It is a trade-off that you want more iterations for the gradient descent so you can produce a better model, but at the same time, you do not want the training to take too long to complete. The number of epochs and the size of a batch can be chosen experimentally by trial and error.\n",
    "\n",
    "The goal of training a model is to ensure it learns a good enough mapping of input data to output classification. It will not be perfect, and errors are inevitable. Usually, you will see the amount of error reducing when in the later epochs, but it will eventually level out. This is called model convergence.\n",
    "\n",
    "The simplest way to build a training loop is to use two nested for-loops, one for epochs and one for batches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c80bafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.6398419141769409\n",
      "Finished epoch 1, latest loss 0.6088632941246033\n",
      "Finished epoch 2, latest loss 0.5943511724472046\n",
      "Finished epoch 3, latest loss 0.5774503946304321\n",
      "Finished epoch 4, latest loss 0.5527340769767761\n",
      "Finished epoch 5, latest loss 0.5176823139190674\n",
      "Finished epoch 6, latest loss 0.49844175577163696\n",
      "Finished epoch 7, latest loss 0.48780590295791626\n",
      "Finished epoch 8, latest loss 0.4818666875362396\n",
      "Finished epoch 9, latest loss 0.477353036403656\n",
      "Finished epoch 10, latest loss 0.4734303653240204\n",
      "Finished epoch 11, latest loss 0.4669589400291443\n",
      "Finished epoch 12, latest loss 0.464997798204422\n",
      "Finished epoch 13, latest loss 0.4605640470981598\n",
      "Finished epoch 14, latest loss 0.45906496047973633\n",
      "Finished epoch 15, latest loss 0.456254243850708\n",
      "Finished epoch 16, latest loss 0.4546455442905426\n",
      "Finished epoch 17, latest loss 0.4562138020992279\n",
      "Finished epoch 18, latest loss 0.45634663105010986\n",
      "Finished epoch 19, latest loss 0.45845329761505127\n",
      "Finished epoch 20, latest loss 0.46203696727752686\n",
      "Finished epoch 21, latest loss 0.46401482820510864\n",
      "Finished epoch 22, latest loss 0.45501649379730225\n",
      "Finished epoch 23, latest loss 0.45923614501953125\n",
      "Finished epoch 24, latest loss 0.4555816352367401\n",
      "Finished epoch 25, latest loss 0.45752325654029846\n",
      "Finished epoch 26, latest loss 0.4530395567417145\n",
      "Finished epoch 27, latest loss 0.4534321427345276\n",
      "Finished epoch 28, latest loss 0.453967809677124\n",
      "Finished epoch 29, latest loss 0.4572487771511078\n",
      "Finished epoch 30, latest loss 0.4460369050502777\n",
      "Finished epoch 31, latest loss 0.45754361152648926\n",
      "Finished epoch 32, latest loss 0.4515988528728485\n",
      "Finished epoch 33, latest loss 0.45192915201187134\n",
      "Finished epoch 34, latest loss 0.4495238959789276\n",
      "Finished epoch 35, latest loss 0.4436779022216797\n",
      "Finished epoch 36, latest loss 0.43944936990737915\n",
      "Finished epoch 37, latest loss 0.4443633556365967\n",
      "Finished epoch 38, latest loss 0.4421570301055908\n",
      "Finished epoch 39, latest loss 0.44084903597831726\n",
      "Finished epoch 40, latest loss 0.4372713267803192\n",
      "Finished epoch 41, latest loss 0.4389289915561676\n",
      "Finished epoch 42, latest loss 0.44170767068862915\n",
      "Finished epoch 43, latest loss 0.4333893060684204\n",
      "Finished epoch 44, latest loss 0.4381856620311737\n",
      "Finished epoch 45, latest loss 0.43984144926071167\n",
      "Finished epoch 46, latest loss 0.43151044845581055\n",
      "Finished epoch 47, latest loss 0.42620760202407837\n",
      "Finished epoch 48, latest loss 0.42599496245384216\n",
      "Finished epoch 49, latest loss 0.4272574484348297\n",
      "Finished epoch 50, latest loss 0.42110803723335266\n",
      "Finished epoch 51, latest loss 0.42253273725509644\n",
      "Finished epoch 52, latest loss 0.41987839341163635\n",
      "Finished epoch 53, latest loss 0.4189061224460602\n",
      "Finished epoch 54, latest loss 0.4160473048686981\n",
      "Finished epoch 55, latest loss 0.41541314125061035\n",
      "Finished epoch 56, latest loss 0.4168238043785095\n",
      "Finished epoch 57, latest loss 0.4175987243652344\n",
      "Finished epoch 58, latest loss 0.4219943881034851\n",
      "Finished epoch 59, latest loss 0.4221732020378113\n",
      "Finished epoch 60, latest loss 0.42311161756515503\n",
      "Finished epoch 61, latest loss 0.421427845954895\n",
      "Finished epoch 62, latest loss 0.4260089695453644\n",
      "Finished epoch 63, latest loss 0.37497660517692566\n",
      "Finished epoch 64, latest loss 0.37238460779190063\n",
      "Finished epoch 65, latest loss 0.3564208447933197\n",
      "Finished epoch 66, latest loss 0.3596435487270355\n",
      "Finished epoch 67, latest loss 0.3308057487010956\n",
      "Finished epoch 68, latest loss 0.3309081792831421\n",
      "Finished epoch 69, latest loss 0.3179444670677185\n",
      "Finished epoch 70, latest loss 0.3189435601234436\n",
      "Finished epoch 71, latest loss 0.3205091059207916\n",
      "Finished epoch 72, latest loss 0.3148374557495117\n",
      "Finished epoch 73, latest loss 0.3185206651687622\n",
      "Finished epoch 74, latest loss 0.32362839579582214\n",
      "Finished epoch 75, latest loss 0.32015228271484375\n",
      "Finished epoch 76, latest loss 0.31230103969573975\n",
      "Finished epoch 77, latest loss 0.3138967752456665\n",
      "Finished epoch 78, latest loss 0.3104628920555115\n",
      "Finished epoch 79, latest loss 0.3051164448261261\n",
      "Finished epoch 80, latest loss 0.3037327527999878\n",
      "Finished epoch 81, latest loss 0.29873034358024597\n",
      "Finished epoch 82, latest loss 0.3002766966819763\n",
      "Finished epoch 83, latest loss 0.2994205951690674\n",
      "Finished epoch 84, latest loss 0.3008159399032593\n",
      "Finished epoch 85, latest loss 0.29815158247947693\n",
      "Finished epoch 86, latest loss 0.29753610491752625\n",
      "Finished epoch 87, latest loss 0.2970491945743561\n",
      "Finished epoch 88, latest loss 0.29522979259490967\n",
      "Finished epoch 89, latest loss 0.29319435358047485\n",
      "Finished epoch 90, latest loss 0.29191693663597107\n",
      "Finished epoch 91, latest loss 0.29441696405410767\n",
      "Finished epoch 92, latest loss 0.29240888357162476\n",
      "Finished epoch 93, latest loss 0.2889978587627411\n",
      "Finished epoch 94, latest loss 0.28777503967285156\n",
      "Finished epoch 95, latest loss 0.283206045627594\n",
      "Finished epoch 96, latest loss 0.2827657163143158\n",
      "Finished epoch 97, latest loss 0.28538432717323303\n",
      "Finished epoch 98, latest loss 0.2824741005897522\n",
      "Finished epoch 99, latest loss 0.2792317271232605\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n",
    "\n",
    "# When this runs, it will print the following:\n",
    "# Finished epoch 0, latest loss 0.6271069645881653\n",
    "# Finished epoch 1, latest loss 0.6056771874427795\n",
    "# Finished epoch 2, latest loss 0.5916517972946167\n",
    "# Finished epoch 3, latest loss 0.5822567939758301\n",
    "# Finished epoch 4, latest loss 0.5682642459869385\n",
    "# Finished epoch 5, latest loss 0.5640913248062134\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad054025",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset. This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy) but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset in the same way you invoked the model in training. This will generate predictions for each input, but then you still need to compute a score for the evaluation. This score can be the same as your loss function or something different. Because you are doing binary classification, you can use accuracy as your evaluation score by converting the output (a floating point in the range of 0 to 1) to an integer (0 or 1) and compare to the label we know.\n",
    "\n",
    "This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e604f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7669270634651184\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae0051",
   "metadata": {},
   "source": [
    "The round() function rounds off the floating point to the nearest integer. The == operator compares and returns a Boolean tensor, which can be converted to floating point numbers 1.0 and 0.0. The mean() function will provide you the count of the number of 1’s (i.e., prediction matches the label) divided by the total number of samples. The no_grad() context is optional but suggested, so you relieve y_pred from remembering how it comes up with the number since you are not going to do differentiation on it.\n",
    "\n",
    "Putting everything together, the following is the complete code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace09fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('C:/Users/Evan Kwan/Desktop/Python/pima-indians-diabetes.csv', delimiter = ',')\n",
    "y = dataset[:,8]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# train the model\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n",
    "\n",
    "# compute accuracy (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55e258",
   "metadata": {},
   "source": [
    "You can copy all the code into your Python file and save it as “pytorch_network.py” in the same directory as your data file “pima-indians-diabetes.csv”. You can then run the Python file as a script from your command line.\n",
    "\n",
    "Running this example, you should see that the training loop progresses on each epoch with the loss with the final accuracy printed last. Ideally, you would like the loss to go to zero and the accuracy to go to 1.0 (e.g., 100%). This is not possible for any but the most trivial machine learning problems. Instead, you will always have some error in your model. The goal is to choose a model configuration and training configuration that achieves the lowest loss and highest accuracy possible for a given dataset.\n",
    "\n",
    "Neural networks are stochastic algorithms, meaning that the same algorithm on the same data can train a different model with different skill each time the code is run. This is a feature, not a bug. The variance in the performance of the model means that to get a reasonable approximation of how well your model is performing, you may need to fit it many times and calculate the average of the accuracy scores. For example, below are the accuracy scores from re-running the example five times:\n",
    "\n",
    "Accuracy: 0.7604166865348816\n",
    "Accuracy: 0.7838541865348816\n",
    "Accuracy: 0.7669270634651184\n",
    "Accuracy: 0.7721354365348816\n",
    "Accuracy: 0.7669270634651184\n",
    "\n",
    "You can see that all accuracy scores are around 77%, roughly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f3e3c",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "You can adapt the above example and use it to generate predictions on the training dataset, pretending it is a new dataset you have not seen before. Making predictions is as easy as calling the model as if it is a function. You are using a sigmoid activation function on the output layer so that the predictions will be a probability in the range between 0 and 1. You can easily convert them into a crisp binary prediction for this classification task by rounding them. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cceb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model(X)\n",
    "\n",
    "# round predictions\n",
    "rounded = predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7675e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternately, you can convert the probability into 0 or 1 to predict crisp classes directly;\n",
    "# for example:\n",
    "\n",
    "# make class predictions with the model\n",
    "predictions = (model(X) > 0.5).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b382fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc0ff022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n",
      "Accuracy 0.7721354365348816\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# The complete example below makes predictions for each example in the dataset, \n",
    "# then prints the input data, predicted class, and expected class for the first five examples\n",
    "# in the dataset.\n",
    "# This code uses a different way of building the model but should functionally be the same as before.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('C:/Users/Evan Kwan/Desktop/Python/pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# define the model\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = PimaClassifier()\n",
    "print(model)\n",
    "\n",
    "# train the model\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# compute accuracy\n",
    "y_pred = model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")\n",
    "\n",
    "# make class predictions with the model\n",
    "predictions = (model(X) > 0.5).int()\n",
    "for i in range(5):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))\n",
    "\n",
    "\n",
    "# After the model is trained, predictions are made for all examples in the dataset,\n",
    "# and the input rows and predicted class value for the first five examples are printed and\n",
    "# compared to the expected class value. You can see that most rows are correctly predicted.\n",
    "# In fact, you can expect about 77% of the rows to be correctly predicted based on your\n",
    "# estimated performance of the model in the previous section.\n",
    "\n",
    "# [6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
    "# [1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
    "# [8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
    "# [1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
    "# [0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fd454",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
